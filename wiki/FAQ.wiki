#summary Frequently Asked Questions

== Acknowledgements ==
I'd like to thank everyone who has submitted bugs, feature requests, or has reviewed the algorithm/implementation.

Special thanks to (in no particular order):
 * Greg Luck, who provided valuable feedback based on his own testing.
 * Adam Zell, who wrote the build, code reviews, and finds interesting papers.
 * Bob Lane, whose performance and concurrency expertise was invaluable when exploring algorithmic ideas.

== Usage ==
* Should I use this data structure?

While a goal is to have a functional implementation that can be used, the primary motivation for its creation was exploratory. It serves as a means to better understand concurrent and lock free algorithms by experimenting with different techinques. A pragmatic approach would be to implement a known algorithm, but this would not be educational and therefore defeats this project's sole purpose.

The practicality of why one would need this data structure is an interesting question. The miss penalty of a cache will far outweigh the overhead of synchronization on a cache hit. If lock contention is the issue then much simpler approaches offer equivilant performance. Example acceptable solutions include (1) an eviction thread that is woken when a threshold is crossed, (2) an eviction queue, (3) a [http://java.sun.com/javase/6/docs/api/java/util/LinkedHashMap.html LinkedHashMap] guarded by a read/write lock, and (4) eviction based on statistical sampling.

If the application realizes a noticable performance gain by using this type of data structure, it most likely indicates that there is a design flaw. As memory is a very expensive resource, a layering of caches (memory, disk, etc) should provide an acceptable miss penalty. The scope of data should be limited when possible, such as through thread-local storage if used only by a single execution flow. Techniques like these should be leveraged so that the application's acceptable performance is not impacted by this type of data structure.

The primary value for using this data structure should be (1) as a performance buffer to provide time to fix the application's architecture; (2) to improve operational cost and aid in capacity planning. By having a higher performance application the costs can be reduced in cooling, server management, hardware, etc. This goal is achieved by aggregating many improvements.

This discussion is not to disway usage of this data structure or others like it. However, from a practical perspective it should have limited value. From a theoretical perspective, it is a fun engineering exercise.

* Which eviction policy is recommended?

The _Second Chance Fifo_ policy provides both an excellent hit rate and concurrency performance. It is recommended as the primary policy.

If the _Least Recently Used_ policy is used then the amount of expected concurrency should be relatively small. This is because each operation requires reordering the list which creates contention at the tail. In practice the cost is acceptable, but in extreme usages this policy will have poor performance characteristics.

* What is the maximum capacity?

A hash table uses a hashing function to spread the entries across segments, each of which consists of a linked list. This requires that each operation searches the segment's link chain, such as to determine the element for retrieval. This provides O(L) performance if the hashing function provides uniform distribution. If there are few hash collisions due to a greater number of segments than entries, the performance is O(1).

The probability of a hash collision can be determined by applying the equation derived from the _birthday problem_ to hashing. In the following equation, *H* is the number of possible outputs and *n* is the number of entries. As Java uses a 32-bit hashcode, *H* is 4.3 billion (2^32).

  [http://concurrentlinkedhashmap.googlecode.com/svn/wiki/images/probability.gif]

|| *N* || *Probability* ||
|| 10,000 || 1% ||
|| 50,000 || 25% ||
|| 100,000 || 69% ||
|| 150,000 || 93% ||
|| 200,000 || 99% ||

A [http://java.sun.com/javase/6/docs/api/java/util/concurrent/ConcurrentHashMap.html ConcurrentHashMap] supports up to 65,536 (2^16) segments. By assuming an uniform distribution across segments then the length of the link chain to search through would grow as follows:

|| *N* || *Links* ||
|| 200,000 || 3 ||
|| 500,000 || 8 ||
|| 1,000,000 || 15 ||
|| 5,000,000 || 76 ||
|| 10,000,000 || 153 ||

At a million or greater entries, the cost of searching the link chain will become unnacceptable. As the hashing function is not truly uniform then the upper bound should probably be at around 200,000 entries. A greater number of entries could be achieved by using a composite view. This composite map would use a different hashing function to select a data map to delegate to. For example, 300+ data maps would be required at 50 million entries to achieve acceptable performance.

An alternative with large maps is to use a skip-list, which provides the properties of a tree with better concurrency characteristics. By delegating to a [http://java.sun.com/javase/6/docs/api/java/util/concurrent/ConcurrentSkipListMap.html ConcurrentSkipListMap] the search becomes O(log n). At over 1.3 million entries the skip-list provides a more efficient data structure.

|| *N* || *Height* ||
|| 200,000 || 18 ||
|| 500,000 || 19 ||
|| 1,000,000 || 20 ||
|| 5,000,000 || 22 ||
|| 10,000,000 || 23 ||


== Development ==
* Why are fields marked volatile with an [http://java.sun.com/javase/6/docs/api/java/util/concurrent/atomic/AtomicReferenceFieldUpdater.html AtomicReferenceFieldUpdater] instead of using an [http://java.sun.com/javase/6/docs/api/java/util/concurrent/atomic/AtomicReference.html AtomicReference]?

This was done based on a [http://cs.oswego.edu/pipermail/concurrency-interest/2007-March/003802.html recommendation] by Doug Lea in regards to [http://java.sun.com/javase/6/docs/api/java/util/concurrent/ConcurrentLinkedQueue.html ConcurrentLinkedQueue].

  The two choices have different tradeoffs. Using fieldUpdaters does have more per-call overhead, but in principle most of it is optimizable away, and on some platforms and some contexts, it often is. Using a separate [http://java.sun.com/javase/6/docs/api/java/util/concurrent/atomic/AtomicReference.html AtomicReference] in essence doubles the length of a list (every second indirection is just a pointer holding the real pointer). And for small nodes as used here, nearly doubles the footprint -- even a one-field object has object header etc overhead, and increases GC overhead. All in all, using fieldUpdaters in this case is the best choice, even though on some programs/platforms it might on average be a  little slower (and on others faster).

  -Doug