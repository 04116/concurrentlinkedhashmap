#summary The algorithmic design of a lock-free linked hash map.
#labels Phase-Design,Featured

= Introduction =
The following describes the algorithm for a concurrent [http://java.sun.com/javase/6/docs/api/java/util/LinkedHashMap.html LinkedHashMap]. Please note that at this time it has not been fully implemented, but is the end goal.

== Buffers ==
The usage of buffers between elements is a key design choice. When removing an element from the middle of the list, two seperate atomic operations must be preformed (update prev->next, next->prev). As atomic operations are not composable, if concurrent removals of sequential elements occurs then the links can become corrupt. The immediate solutions are to fix links by a backtracking algorithm (error-prone), lock all three nodes, or mark the node as 'dead' and allow it to be removed by the eviction policy. The latter, while simple and safe, consumes a slot in the cache and can be modeled as an eviction queue.

The buffers strategy allows for isolating each element so that operations on one does not affect another. Each element owns one side of a shared buffer and is guaranteed that no concurrent operations will be performed. The element can be safely removed and the extra buffer removed by _swinging_ its neighbor's previous pointer across it.

== Elements ==
An element is the {key, value} pair, plus any additional information that is useful for the eviction algorithm (e.g. frequency).

== Hash Table ==
The hash table allows for O(1) access to a cached entry. The table associates the key to an element, which is linked in the list. This can be thought of as cursor into the list data structure, allowing manipulations in O(1) efficiency.

== Cache ==
The capacity is specified by the client and the size is the number of elements linked on the list. The size field is updated on each insertion or removal from the list. After an element is inserted into the list the state is evaluated to determine whether the cache has overflowed (size > capacity) and, if so, an eviction is triggered.

----

= Insertion =
Insertion is the most common case for an LRU cache. Whenever an element is accessed, it is removed from the list and inserted to the tail.

=== First insertion ===
When the map is empty, the head and tail sentinel nodes are linked together. The sentinel nodes are buffers that can never be removed.

[http://concurrentlinkedhashmap.googlecode.com/svn/wiki/images/design/empty.png]

An insertion always appends to the tail of the list. To ensure that no two elements touch, a buffer-element bundle is inserted.
 # Insert entry into hash table
 # Append buffer-element to list

[http://concurrentlinkedhashmap.googlecode.com/svn/wiki/images/design/first-insertion.png]

=== Subsequent insertions ===
After three insertions, the list is now:

[http://concurrentlinkedhashmap.googlecode.com/svn/wiki/images/design/initial.png]

A fourth entry is now added. This follows the same logic as for the first.
 # Insert entry into hash table
 # Append buffer-element to list

[http://concurrentlinkedhashmap.googlecode.com/svn/wiki/images/design/subsequent-insertion.png]

----

= Eviction =
When the capacity is reached, an entry must be evicted. This is done by removing the first element from the list and then from the map. Since elements hold the key, this can be done in the map's O(1) time.

If we use inserted bundles as of element-buffer pairs, the head may point to either a buffer or an element. It uses a polling mechanism, similarly to a queue, to extract the first node. If it is a buffer, it retries.

[http://concurrentlinkedhashmap.googlecode.com/svn/wiki/images/design/buffer-eviction.png]

If the head extracted an element node, the map's entry is removed using the key inside the element. The size is then decremented.

[http://concurrentlinkedhashmap.googlecode.com/svn/wiki/images/design/element-eviction.png]

The algorithm is as follows:
 # Extract first node on list
 # Evaluate node:
   * Buffer: Retry eviction
   * Element: Remove from map and decrement size

=== Optimization ===
If we inserted bundles as element-buffer pairs, the head always points to either the tail or an element. If an element, the key is retrieved and the standard removal process attempted. If unsuccessful due to a concurrent removal, the eviction can retry until either it succeeds, the tail is found, or another thread has evicted such that the capacity threshold is no longer crossed.

----

= Removal =
In an LRU cache, removal is a common operation so that an element is shifted to the tail of the list. This maintains a least-recently-used order.

As the map associates the key to a cursor into the list, removal is O(1) in both data structures. The first step is to remove the element:
 # Remove from the map.
 # Evaluate result
   * If null, entry not found
   * If element:
     # Decrement size
     # Unlink it from the list

[http://concurrentlinkedhashmap.googlecode.com/svn/wiki/images/design/remove-middle-1.png]

Once the element is unlinked, an extra buffer remains in the list. This can easily be removed by swinging the pointer _next->prev_ to have the value at _prev->next_. This _swinging_ allows for concurrent removals to not interfere with each other.

[http://concurrentlinkedhashmap.googlecode.com/svn/wiki/images/design/remove-middle-2.png]

After removal, the list has been brought to the expected state.

[http://concurrentlinkedhashmap.googlecode.com/svn/wiki/images/design/remove-middle-3.png]

=== Optimization ===
Once the entry is removed from the map, no concurrent operation can lookup a cursor to that element. Therefore, breaking the associations of the element with its buffers is unnessary. Instead, the bundle could be removed together rather than seperately. This simplifies and speeds up the operation.